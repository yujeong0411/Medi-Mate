import re
import urllib.parse
from typing import List, Dict

def is_valid_content(content: str) -> bool:
    """ìœ íš¨í•œ ë‚´ìš©ì¸ì§€ í™•ì¸"""
    # null ê°’ ì²´í¬
    if content is None:
        return False
        
    if not content or not isinstance(content, str):
        return False
    
    content = content.strip()
    if len(content) < 4:
        return False
    
    # ì˜ë¯¸ ì—†ëŠ” í…ìŠ¤íŠ¸ íŒ¨í„´
    meaningless = ['í•´ë‹¹ì—†ìŒ', 'ì—†ìŒ', '-', 'N/A', 'ì •ë³´ì—†ìŒ']
    content_lower = content.lower()

    for pattern in meaningless:
        if pattern.lower() in content_lower:
            return False
        
    return True

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ì œ"""
    if not text:
        return ""
    
    # HTML ì œê±°
    text = re.sub(r'<[^>]+>', '', text)

    # HTML ì—”í‹°í‹° ë³€í™˜
    # '&nbsp;'  Non-Breaking Space â†’ ê³µë°± (' ')
    # '&lt;'     Less Than â†’ '<' ê¸°í˜¸
    # '&gt;'     Greater Than â†’ '>' ê¸°í˜¸  
    # '&amp;'    Ampersand â†’ '&' ê¸°í˜¸
    # '&quot;'   Quotation â†’ '"' ê¸°í˜¸
    # '&#39;'    Apostrophe â†’ "'" ê¸°í˜¸
    replacements = {
        '&nbsp;': ' ', '&lt;': '<', '&gt;': '>',
        '&amp;': '&', '&quot;': '"', '&#39;': "'"
    }

    for old, new in replacements.items():
        text = text.replace(old, new)

    # ê³µë°± ì²˜ë¦¬
    text = re.sub(r'\s+', ' ', text)

    return text.strip()

def extract_core_effects(text: str) -> str:
    """íš¨ëŠ¥ì—ì„œ í•µì‹¬ ì§ˆí™˜/ì¦ìƒ íŒ¨í„´ë§Œ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 5:
        return ""
    
    # ì§ˆí™˜/ì¦ìƒ íŒ¨í„´ë“¤ì„ ì •ê·œì‹ìœ¼ë¡œ ì¶”ì¶œ
    patterns = [
        r'[ê°€-í£]{2,}ì¦',          # ~ì¦ (ê¸°ëŠ¥ë¬´ë ¥ì¦, ê²°í•ì¦ ë“±)
        r'[ê°€-í£]{2,}ì—¼',          # ~ì—¼ (ê¸°ê´€ì§€ì—¼, ìœ„ì—¼ ë“±) 
        r'[ê°€-í£]{2,}í†µ',          # ~í†µ (ê·¼ìœ¡í†µ, ê´€ì ˆí†µ ë“±)
        r'[ê°€-í£]{2,}ë¶ˆëŸ‰',        # ~ë¶ˆëŸ‰ (ì†Œí™”ë¶ˆëŸ‰ ë“±)
        r'[ê°€-í£]{2,}ì¥ì• ',        # ~ì¥ì•  (ìˆœí™˜ì¥ì•  ë“±)
        r'[ê°€-í£]{2,}ì¤‘ë…',        # ~ì¤‘ë… (ì•½ë¬¼ì¤‘ë… ë“±)
        r'ìŠµì§„|í”¼ë¶€ì—¼|í™”ìƒ|ì—´ìƒ',   # í”¼ë¶€ ê´€ë ¨
        r'ê°ë‹´|ê°€ë˜|ê¸°ì¹¨',         # í˜¸í¡ê¸° ê´€ë ¨
        r'ë³€ë¹„|ì„¤ì‚¬|êµ¬í† |êµ¬ì—­',     # ì†Œí™”ê¸° ê´€ë ¨
        r'ì§„í†µ|ì†Œì—¼|í•­ì—¼',         # ì¹˜ë£Œ íš¨ê³¼
        r'[ê°€-í£]{2,}ì €ë¦¼',     # ìˆ˜ì¡±ì €ë¦¼ ë“±
        r'[ê°€-í£]{2,}ëƒ‰ì¦',     # ìˆ˜ì¡±ëƒ‰ì¦ ë“±  
        r'[ê°€-í£]{2,}ë¶€ì „',     # ë¶„ë¹„ë¶€ì „ ë“±
        r'ë¹„íƒ€ë¯¼\s*[A-Z].*?ê²°í•ì¦',  # ë¹„íƒ€ë¯¼ ê²°í•ì¦
        r'ìˆœí™˜.*?ì¥ì• ',        # ìˆœí™˜ì¥ì• 
    ]
    
    found_terms = []
    for pattern in patterns:
        matches = re.findall(pattern, text)
        found_terms.extend(matches)
    
    if found_terms:
        # ì¤‘ë³µ ì œê±°í•˜ê³  ìµœëŒ€ 5ê°œ
        unique_terms = list(dict.fromkeys(found_terms))[:5]
        return ", ".join(unique_terms)
    else:
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì²« ë¬¸ì¥ì˜ í•µì‹¬ ë¶€ë¶„ë§Œ
        cleaned = re.sub(r'ì´ ì•½ì€\s*', '', text)
        cleaned = re.sub(r'ì— ì‚¬ìš©í•©ë‹ˆë‹¤.*', '', cleaned)
        return cleaned.strip()[:50]

def extract_core_dosage(text: str) -> str:
    """ë³µìš©ë²•ì—ì„œ í•µì‹¬ ìš©ë²• íŒ¨í„´ë§Œ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 5:
        return ""
    
    results = []
    
    # ì„±ì¸ ìš©ë²• íŒ¨í„´ 
    adult_patterns = [
        r'ì„±ì¸.*?(1íšŒ\s*[\d~.-]+[ê°€-í£mgml]*.*?\d+ì¼\s*\d+íšŒ)',
        r'ì„±ì¸.*?(\d+ì¼\s*\d+íšŒ.*?[\d~.-]+[ê°€-í£mgml]*)',
        r'ì„±ì¸.*?(1íšŒ\s*[\d~.-]+[ê°€-í£mgml/]*(?:\s*ì”©)?)',  # "1íšŒ 1í¬ì”©" ì¶”ê°€
        r'ì„±ì¸.*?(\d+ì¼\s*\d+[~-]?\d*[ë§¤ì •í¬ìº¡ìŠë°©ìš¸mlmg])',

    ]
    
    for pattern in adult_patterns:
        match = re.search(pattern, text)
        if match:
            dosage = re.sub(r'\s+', ' ', match.group(1)).strip()
            results.append(f"ì„±ì¸ {dosage}")
            break
    
    # ì†Œì•„/ì–´ë¦°ì´ ìš©ë²• íŒ¨í„´
    child_patterns = [
        r'(ì†Œì•„|ì–´ë¦°ì´|ìœ ì•„).*?(\d+ì„¸.*?\d+[íšŒë§¤ì •í¬mlmg].*?\d+[ê°€-í£]*)',
        r'(\d+ì„¸\s*ì´ìƒ.*?\d+[íšŒë§¤ì •í¬mlmg].*?\d+[ê°€-í£]*)',
        r'(\d+ì„¸\s*ë¯¸ë§Œ.*?\d+[íšŒë§¤ì •í¬mlmg].*?\d+[ê°€-í£]*)',
        r'(ë§Œ\s*\d+ì„¸.*?1íšŒ.*?[\d/]+[ê°€-í£mgml]*)'
    ]
    
    for pattern in child_patterns:
        match = re.search(pattern, text)
        if match:
            dosage = re.sub(r'\s+', ' ', match.group().strip())
            results.append(dosage)
            break

    # ì¼ë°˜ì ì¸ ìš©ë²• íŒ¨í„´ (ì„±ì¸ í‘œê¸°ê°€ ì—†ëŠ” ê²½ìš°)
    if not results:  # ì„±ì¸/ì†Œì•„ íŒ¨í„´ì´ ì—†ì„ ë•Œë§Œ
        general_patterns = [
            r'1ì¼\s*\d+[~-]?\d*íšŒ.*?í™˜ë¶€',
            r'1íšŒ\s*\d+[~-]?\d*ë°©ìš¸.*?\d+ì¼\s*\d+íšŒ',
            r'1ì¼\s*ìˆ˜íšŒ.*?í™˜ë¶€',
            r'1ì¼\s*\d+[~-]?\d*íšŒ.*?ì ë‹¹ëŸ‰',
            r'1ì¼\s*\d+[~-]?\d*íšŒ.*?í™˜ë¶€.*?ë¶™ì…ë‹ˆë‹¤',
        ]
        
        for pattern in general_patterns:
            match = re.search(pattern, text)
            if match:
                results.append(match.group().strip())
                break
    
    # ë³µìš©/ì‚¬ìš© ì‹œê¸° íŒ¨í„´
    timing_patterns = [
        r'ì‹ì „|ì‹í›„|ì‹ê°„',
        r'ì•„ì¹¨|ì ì‹¬|ì €ë…|ì·¨ì¹¨ì „',
        r'í™˜ë¶€|ìƒì²˜ë¶€ìœ„',
        r'ìˆ˜íšŒ|ì—¬ëŸ¬\s*ì°¨ë¡€',
        r'ì ì•ˆ|ë„í¬|ë°”ë¥´|ë¶™'
    ]
    
    timing = []
    for pattern in timing_patterns:
        matches = re.findall(pattern, text)
        timing.extend(matches)
    
    if timing:
        unique_timing = list(dict.fromkeys(timing))[:3]
        results.append(f"{'/'.join(unique_timing)}")

    # ìš©ëŸ‰ ì œí•œ íŒ¨í„´
    limit_patterns = [
        r'(\d+[ë§¤ì •í¬ml]\s*ì´ìƒ.*?ì‚¬ìš©í•˜ì§€)',
        r'(\d+ì¼\s*ì´ìƒ.*?ì‚¬ìš©í•˜ì§€)',
        r'ìµœëŒ€.*?(\d+[ê°€-í£]*)',
    ]

    for pattern in limit_patterns:
        match = re.search(pattern, text)
        if match:
            results.append(f"ì œí•œ: {match.group(1)}")
            break
    
    if results:
        return "; ".join(results)
    else:
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ í•µì‹¬ ì •ë³´ë§Œ
        # ìˆ«ì+ë‹¨ìœ„ íŒ¨í„´ (ë§¤, ì •, í¬, mg, ml ë“±)
        dosage_numbers = re.findall(r'\d+[ê°€-í£]*\s*\d+íšŒ|\d+ì¼\s*\d+[ë§¤ì •í¬ml]|1íšŒ\s*\d+[ê°€-í£]*|\d+[ë§¤ì •í¬ìº¡ìŠë°©ìš¸mgml]', text)
        if dosage_numbers:
            return "; ".join(dosage_numbers[:3])
        return text[:50]
    
def extract_core_warnings(text: str) -> str:
    """ì£¼ì˜ì‚¬í•­ì—ì„œ í•µì‹¬ ê¸ˆê¸°/ì£¼ì˜ íŒ¨í„´ë§Œ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 5:
        return ""
    
    # ê¸ˆê¸° ëŒ€ìƒ íŒ¨í„´ (ë” ì •í™•í•œ ë§¤ì¹­)
    contraindication_patterns = [
        r'\d+ì„¸\s*ë¯¸ë§Œ.*?ìœ ì•„',
        r'\d+ê°œì›”\s*ì´í•˜.*?ìœ ì•„',
        r'ì„ë¶€|ì„ì‹ .*?ì—¬ì„±',
        r'ìˆ˜ìœ ë¶€',
        r'ê³¼ë¯¼ì¦\s*í™˜ì',
        r'í”¼ë¶€\s*ê°ì—¼ì¦',
        r'ê³ ë§‰\s*ì²œê³µ',
        r'ê¶¤ì–‘.*?í™˜ì',
        r'í™”ìƒ.*?í™˜ì',
        r'ê³ ë ¹ì',
        r'[ê°€-í£]*ì¥ì• .*?í™˜ì',  # ì‹ ì¥ì¥ì• , ê°„ì¥ì•  ë“± ì¶”ê°€
        r'[ê°€-í£]*í˜ˆì¦.*?í™˜ì',  # ê³ ë§ˆê·¸ë„¤ìŠ˜í˜ˆì¦ ë“± ì¶”ê°€
    ]
    
    # ì‚¬ìš© ì œí•œ íŒ¨í„´
    restriction_patterns = [
        r'ì•ˆê³¼ìš©.*?ì‚¬ìš©í•˜ì§€',
        r'ì™¸ìš©.*?ì‚¬ìš©',
        r'ì¥ê¸°ê°„.*?ì‚¬ìš©í•˜ì§€',
        r'\d+ì¼\s*ì´ë‚´.*?ì œí•œ',
        r'ì¹˜ë£Œ.*?ëª©ì .*?ì‚¬ìš©í•˜ì§€',
        r'ì˜ì‚¬.*?ê°ë….*?ì—†ì´.*?ì‚¬ìš©í•˜ì§€',
    ]
    
    found_warnings = []
    
    # ê¸ˆê¸° ëŒ€ìƒ ì¶”ì¶œ
    for pattern in contraindication_patterns:
        matches = re.findall(pattern, text)
        if matches:
            found_warnings.extend(matches[:2])  # ìµœëŒ€ 2ê°œ
    
    # ì‚¬ìš© ì œí•œ ì¶”ì¶œ
    for pattern in restriction_patterns:
        matches = re.findall(pattern, text)
        if matches:
            found_warnings.extend(matches[:2])  # ìµœëŒ€ 2ê°œ

    # í•µì‹¬ ì§ˆí™˜ëª… ì¶”ì¶œ (ê´„í˜¸ ì•ˆ ë‚´ìš©)
    disease_pattern = r'\(([^)]{3,15})\)'  # 3-15ê¸€ì ê´„í˜¸ ë‚´ìš©
    diseases = re.findall(disease_pattern, text)
    if diseases:
        # ì£¼ìš” ì§ˆí™˜ëª…ë§Œ ì„ ë³„ (ë„ˆë¬´ ë§ìœ¼ë©´ 3ê°œê¹Œì§€)
        major_diseases = [d for d in diseases if len(d) <= 8][:3]
        if major_diseases:
            found_warnings.append(f"ê¸ˆê¸°ì§ˆí™˜: {', '.join(major_diseases)}")
    
    if found_warnings:
        unique_warnings = list(dict.fromkeys(found_warnings))[:4]
        return "; ".join(unique_warnings)
    else:
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ ì²« ë¬¸ì¥ì˜ í•µì‹¬ë§Œ
        # "ë§ˆì‹­ì‹œì˜¤" ì•ê¹Œì§€ë§Œ ì¶”ì¶œ
        match = re.search(r'^(.*?ìƒì˜í•˜ì‹­ì‹œì˜¤)', text)
        if match:
            return match.group(1)[:80]
        
        # 2ìˆœìœ„: "ë§ˆì‹­ì‹œì˜¤" ì•ê¹Œì§€ë§Œ ì¶”ì¶œ
        match = re.search(r'^(.*?ë§ˆì‹­ì‹œì˜¤)', text)
        if match:
            return match.group(1)[:80]
        
        sentences = re.split(r'[.!]', text)
        if sentences:
            return sentences[0].strip()[:60]
        return text[:60]
    
def extract_core_interactions(text: str) -> str:
    """ìƒí˜¸ì‘ìš©ì—ì„œ ì•½ë¬¼ëª…ê³¼ ì£¼ì˜ì‚¬í•­ íŒ¨í„´ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 5:
        return ""
    
    # ì•½ë¬¼ ë¶„ë¥˜/ê³„ì—´ íŒ¨í„´ (ìš°ì„ ìˆœìœ„)
    drug_class_patterns = [
        r'ì—ìŠ¤íŠ¸ë¡œê².*?í”¼ì„ì•½',
        r'ë¹„íƒ€ë¯¼\s*[A-Z]',
        r'í•­ì‘ê³ ì œ',
        r'ë‹¹ë‡¨ë³‘ì œ',
        r'ë¹„ë§Œì¹˜ë£Œì œ',
        r'ì‚¬í•˜ì œ',
        r'ì² ë¶„ì œ',
        r'ì œì‚°ì œ',
        r'ì§€ì§ˆ.*?ì•½ë¬¼',
        r'í”¼ì„ì•½',
        r'í•­ìƒì œ',
    ]
    
    # ì¼ë°˜ì ì¸ ì•½ë¬¼ëª… íŒ¨í„´ (í•œê¸€ + ì˜ë¬¸)
    general_drug_patterns = [
        r'[ê°€-í£]{3,8}(?:ì •|ìº¡ìŠ|ì•¡|í¬ë¦¼|ì—°ê³ )',  # í•œê¸€ì•½ë¬¼ëª…+ì œí˜•
        r'[A-Za-z]{4,12}',  # ì˜ë¬¸ ì•½ë¬¼ëª… (4-12ê¸€ì)
        r'[ê°€-í£]{2,6}(?:ì•„ë¯¸ë“œ|ë§ˆì´ì‹ |ì½œ)',  # ~ì•„ë¯¸ë“œ, ~ë§ˆì´ì‹  ê³„ì—´
    ]
    
    # ê´„í˜¸ ì•ˆ ì„¤ëª… íŒ¨í„´ (ë¶€ê°€ ì •ë³´)
    description_pattern = r'\(([^)]{3,15})\)'
    
    found_drugs = []
    # 1ìˆœìœ„: ì•½ë¬¼ ë¶„ë¥˜ ì¶”ì¶œ
    for pattern in drug_class_patterns:
        matches = re.findall(pattern, text)
        found_drugs.extend(matches)
    
    # 2ìˆœìœ„: ì¼ë°˜ ì•½ë¬¼ëª… íŒ¨í„´ (ë¶„ë¥˜ê°€ ë¶€ì¡±í•  ë•Œë§Œ)
    if len(found_drugs) < 3:
        for pattern in general_drug_patterns:
            matches = re.findall(pattern, text)
            # ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸
            filtered_matches = [m for m in matches if m not in ['ì‚¬ìš©', 'ë³µìš©', 'í•¨ê»˜', 'ê²½êµ¬', 'í¬í•¨']]
            found_drugs.extend(filtered_matches)

    # 3ìˆœìœ„: ê´„í˜¸ ì•ˆ ì¤‘ìš” ì •ë³´
    descriptions = re.findall(description_pattern, text)
    important_descriptions = [d for d in descriptions if any(keyword in d for keyword in ['ìš©', 'ì œ', 'ì•½', 'ì„±'])]
    found_drugs.extend(important_descriptions[:2])
    
    # ìƒí˜¸ì‘ìš© ì§€ì‹œì‚¬í•­ í™•ì¸
    if 'í•¨ê»˜' in text and ('ë§ˆì‹­ì‹œì˜¤' in text or 'ìƒì˜' in text):
        action_suffix = "ë³‘ìš©ì£¼ì˜"
    else:
        action_suffix = "ìƒí˜¸ì‘ìš©"
    
    if found_drugs:
        unique_drugs = list(dict.fromkeys(found_drugs))[:4]  # ìµœëŒ€ 4ê°œ
        return f"{', '.join(unique_drugs)} {action_suffix}"
    else:
        # ëª¨ë“  íŒ¨í„´ ì‹¤íŒ¨ì‹œ
        if 'í•¨ê»˜' in text:
            return "ë‹¤ìˆ˜ ì•½ë¬¼ê³¼ ë³‘ìš©ì£¼ì˜"
        return text[:40]
    
def extract_core_side_effects(text: str) -> str:
    """ë¶€ì‘ìš©ì—ì„œ ì¦ìƒ íŒ¨í„´ë§Œ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 5:
        return ""
    
    # ë¶€ì‘ìš© ì¦ìƒ íŒ¨í„´
    symptom_patterns = [
        r'[ê°€-í£]{2,}ê°',         # ~ê° (ì—´ê°, ì†Œì–‘ê° ë“±)
        r'[ê°€-í£]{2,}ì¦',         # ~ì¦ (ê°€ë ¤ì›€ì¦ ë“±)
        r'ë°œì§„|ê°€ë ¤ì›€|ë¶€ì¢…|ì¶©í˜ˆ',     # í”¼ë¶€ ì¦ìƒ
        r'êµ¬ì—­|êµ¬í† |ì„¤ì‚¬|ë³€ë¹„',       # ì†Œí™”ê¸° ì¦ìƒ  
        r'ì¡¸ìŒ|ì–´ì§€ëŸ¬ì›€|ë‘í†µ|í”¼ë¡œ',   # ì‹ ê²½ê³„ ì¦ìƒ
        r'ì‹¬ê³„í•­ì§„|í˜¸í¡ê³¤ë€',        # ìˆœí™˜ê¸° ì¦ìƒ
        r'ì‘ì—´ê°|ìê·¹ê°|ë”°ë”',       # ìê·¹ ì¦ìƒ
    ]
    
    found_symptoms = []
    for pattern in symptom_patterns:
        matches = re.findall(pattern, text)
        found_symptoms.extend(matches)
    
    if found_symptoms:
        # ì¤‘ë³µ ì œê±° ë¡œì§ ê°œì„ 
        unique_symptoms = []
        seen = set()
        for symptom in found_symptoms:
            # "ê°€ë ¤ì›€"ê³¼ "ê°€ë ¤ì›€ì¦" ì¤‘ë³µ ì²˜ë¦¬
            normalized = symptom.replace('ì¦', '').replace('ê°', '')
            if normalized not in seen:
                seen.add(normalized)
                unique_symptoms.append(symptom)
        
        return ", ".join(unique_symptoms[:6])
    else:
        # íŒ¨í„´ ë§¤ì¹­ ì‹¤íŒ¨ì‹œ í•µì‹¬ ë¶€ë¶„ë§Œ
        cleaned = re.sub(r'ë“œë¬¼ê²Œ\s*', '', text)
        cleaned = re.sub(r'ë‚˜íƒ€ë‚˜ëŠ”\s*ê²½ìš°.*', '', cleaned)
        cleaned = re.sub(r'ë³µìš©ì„\s*ì¦‰ê°\s*ì¤‘ì§€.*', '', cleaned)
        return cleaned.strip()[:50]

def extract_core_storage(text: str) -> str:
    """ë³´ê´€ë²•ì—ì„œ ë³´ê´€ ì¡°ê±´ íŒ¨í„´ë§Œ ì¶”ì¶œ"""
    if not text or len(text.strip()) < 5:
        return ""
    
    # ë³´ê´€ ì¡°ê±´ íŒ¨í„´
    storage_patterns = [
        r'ì‹¤ì˜¨.*?ë³´ê´€',
        r'ëƒ‰ì¥.*?ë³´ê´€', 
        r'ìŠµê¸°.*?í”¼í•´',
        r'ë¹›.*?í”¼í•´',
        r'ì§ì‚¬ê´‘ì„ .*?í”¼í•´',
        r'ì–´ë¦°ì´.*?ì†.*?ë‹¿ì§€.*?ì•ŠëŠ”.*?ê³³',
        r'\d+ë„.*?ë³´ê´€',
    ]
    
    storage_conditions = []
    for pattern in storage_patterns:
        matches = re.findall(pattern, text)
        storage_conditions.extend(matches)
    
    if storage_conditions:
        unique_conditions = list(dict.fromkeys(storage_conditions))[:3]
        return "; ".join(unique_conditions)
    else:
        return text[:40]
    
# í†µí•© í•µì‹¬ ì¶”ì¶œ í•¨ìˆ˜
def extract_all_core_info(document: Dict) -> Dict:
    """ë¬¸ì„œì˜ ëª¨ë“  í•„ë“œì—ì„œ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
    core_document = document.copy()
    
    # ê° í•„ë“œë³„ ì •ê·œì‹ ì¶”ì¶œ ì ìš©
    field_extractors = {
        'íš¨ê³¼': extract_core_effects,
        'ë³µìš©ë²•': extract_core_dosage, 
        'ì£¼ì˜ì‚¬í•­': extract_core_warnings,
        'ìƒí˜¸ì‘ìš©': extract_core_interactions,
        'ë¶€ì‘ìš©': extract_core_side_effects,
        'ë³´ê´€ë²•': extract_core_storage
    }
    
    for field, extractor in field_extractors.items():
        if field in core_document and core_document[field]:
            original = core_document[field]
            extracted = extractor(original)
            if extracted:
                core_document[field] = extracted
    
    return core_document

# ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„± í•¨ìˆ˜ 
def create_embedding_content(document: Dict) -> str:
    """ë¬¸ì„œì—ì„œ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ë™ì  ìƒì„±"""
    product_name = document['product_name']
    content_parts = [f"ì•½ë¬¼: {product_name}"]

    # ì„±ë¶„ëª… ì¶”ì¶œ (ê´„í˜¸ ì•ˆì˜ ë‚´ìš©)
    if '(' in product_name and ')' in product_name:
        ingredient = product_name.split('(')[1].split(')')[0]
        content_parts.append(f"ì„±ë¶„: {ingredient}")

    # ì•½ë¬¼ ì •ë³´ í•„ë“œë“¤ ì¶”ê°€ (ìˆœì„œ ì¤‘ìš”)
    info_fields = [
        'íš¨ê³¼',
        'ë³µìš©ë²•', 
        'ì£¼ì˜ì‚¬í•­',
        'ìƒí˜¸ì‘ìš©',
        'ë¶€ì‘ìš©',
        'ë³´ê´€ë²•'
    ]

    for field in info_fields:
        if field in document and document[field]:
            content_parts.append(f"{field}: {document[field]}")
    
    return "\n".join(content_parts)

def item_to_documents(item: Dict, search_drug: str = None) -> List[Dict]:
    """API ì‘ë‹µ ì•„ì´í…œì„ ë¬¸ì„œë¡œ ë³€í™˜"""

    product_name = item.get('itemName', '').strip()
    company_name = item.get('entpName', '').strip()

    if not product_name:
        return []
    
    # ì„±ë¶„ëª… ì¶”ì¶œ (URLìš©)
    ingredient_name = ""
    if '(' in product_name and ')' in product_name:
        ingredient_name = product_name.split('(')[1].split(')')[0]
    
    # URL ìƒì„± - ì„±ë¶„ëª… ìš°ì„ , ì—†ìœ¼ë©´ ì œí’ˆëª…
    search_keyword = ingredient_name if ingredient_name else product_name
    drug_url = f"https://nedrug.mfds.go.kr/search?keyword={urllib.parse.quote(search_keyword)}"

    # ê¸°ë³¸ ë¬¸ì„œ êµ¬ì¡°
    document = {
        "drug_name": search_drug or product_name,
        "product_name": product_name,
        "company_name": company_name,
        "source": f"ì‹ì•½ì²˜ ì˜ì•½í’ˆê°œìš”ì •ë³´ - {product_name}",
        "url": drug_url,
        "category": "í†µí•©ì•½ë¬¼ì •ë³´"
    }
    
    # ì§ì ‘ í•„ë“œ ë§¤í•‘ (ê²€ìƒ‰ í‚¤ì›Œë“œ í’ë¶€í™”)
    field_mapping = {
        'efcyQesitm': 'íš¨ê³¼',
        'useMethodQesitm': 'ë³µìš©ë²•',
        'atpnQesitm': 'ì£¼ì˜ì‚¬í•­',
        'intrcQesitm': 'ìƒí˜¸ì‘ìš©',
        'seQesitm': 'ë¶€ì‘ìš©',
        'depositMethodQesitm': 'ë³´ê´€ë²•'
    }

    # ê° í•„ë“œ ì •ë³´ë¥¼ ì§ì ‘ ë¬¸ì„œì— ì¶”ê°€
    added_fields = 0

    for api_field, doc_field in field_mapping.items():
        content = item.get(api_field, '')

        if is_valid_content(content):
            cleaned_content = clean_text(content)

            if len(cleaned_content) > 15:
                document[doc_field] = cleaned_content
                added_fields += 1

    # ìœ íš¨í•œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if added_fields == 0:
        return []
    
    # ğŸ†• ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ í•µì‹¬ ì¶”ì¶œ
    core_document = extract_all_core_info(document)
    return [core_document]
