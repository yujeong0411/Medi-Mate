import os
import json
import faiss
import numpy as np
import urllib.parse
from datetime import datetime
from typing import List, Dict
from openai import OpenAI
from dotenv import load_dotenv
from kfda_data_handler import get_data_handler, search_medical_data
from direct_embedder import DirectEmbedder

# Responses APIê°€ ìƒˆë¡œ ë‚˜ì™”ì§€ë§Œ, ì•ˆì •ì„±ì„ ìœ„í•´ Chat Completions API ì‚¬ìš©

load_dotenv()

class MedicalRAGSystem:
    """ì˜êµ¬ ì €ì¥ ê°€ëŠ¥í•œ FAISS ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""

    def __init__(self, data_dir="./data"):
        # ê²½ë¡œ ì„¤ì •
        self.data_dir = data_dir
        self.index_path = os.path.join(data_dir, "medical_docs.index")
        self.documents_path = os.path.join(data_dir, "documents.json")
        self.last_update_path = os.path.join(data_dir, "last_update.txt")

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(data_dir, exist_ok=True)

        # OPENAI ì„¤ì •
        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

        # ì„ë² ë”© ëª¨ë¸
        print("í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        self.embedder = DirectEmbedder('jhgan/ko-sroberta-multitask')
        print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # ì‹ì•½ì²˜ ë°ì´í„° í•¸ë“¤ëŸ¬
        self.data_handler = get_data_handler()

        # FAISS ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        self.index = None
        self.documents = []

        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_system()

        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

    def _initialize_system(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”: ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ë˜ëŠ” ìƒˆë¡œ ìƒì„±"""
        if self._load_existing_index():
            print("ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
        else:
            print("ë²¡í„° DBê°€ ì—†ìŠµë‹ˆë‹¤. ë³„ë„ êµ¬ì¶• ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")
            # ë¹ˆ ì¸ë±ìŠ¤ë¡œ ì‹œì‘
            self.index = None
            self.documents = []

    def _load_existing_index(self) -> bool:
        """ê¸°ì¡´ ì¸ë±ìŠ¤ì™€ ë©”íƒ€ë°ì´í„° ë¡œë“œ"""

        try:
            if os.path.exists(self.index_path) and os.path.exists(self.documents_path):
                # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
                self.index = faiss.read_index(self.index_path)
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                with open(self.documents_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.documents = data['documents']
                
                print(f"ê¸°ì¡´ ë°ì´í„° ë¡œë“œ: {len(self.documents)}ê°œ ë¬¸ì„œ")
                return True
                
        except Exception as e:
            print(f"ê¸°ì¡´ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")

        return False

    def _rebuild_index(self, documents: List[Dict]):
        """ì¸ë±ìŠ¤ ì¬êµ¬ì¶•"""
        print(f"{len(documents)}ê°œ ë¬¸ì„œë¡œ FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        
        # ì„ë² ë”© ìƒì„±
        contents = [doc["content"] for doc in documents]
        embeddings = self.embedder.encode(contents)
        
        # FAISS ì¸ë±ìŠ¤ ìƒì„±
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)
        
        # L2 ì •ê·œí™” í›„ ì¶”ê°€
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        
        
        self.documents = documents
        
        # ë””ìŠ¤í¬ì— ì €ì¥
        self._save_to_disk()
        print("FAISS ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ")

    def _save_to_disk(self):
        """ì¸ë±ìŠ¤ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        try:
            # FAISS ì¸ë±ìŠ¤ ì €ì¥
            faiss.write_index(self.index, self.index_path)
            
            data = {
                'documents': self.documents,
                'last_updated': datetime.now().isoformat()
            }
            with open(self.documents_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            
            # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ ì €ì¥
            with open(self.last_update_path, 'w') as f:
                f.write(datetime.now().isoformat())
                
            print("ë””ìŠ¤í¬ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"ë””ìŠ¤í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            
    def search_documents(self, query: str, top_k: int = 3) -> List[Dict]:
        """ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰ - ë²¡í„° ê²€ìƒ‰"""
        if self.index is None:
            print("ê²€ìƒ‰ ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        # 1. ì¿¼ë¦¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        query_embedding = self.embedder.encode([query])
        faiss.normalize_L2(query_embedding)
        
        # 2. FAISSì—ì„œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° + ê²€ìƒ‰
        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
        
        # 3. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²°ê³¼ ë°˜í™˜
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx != -1 and idx < len(self.documents):  # ìœ íš¨í•œ ì¸ë±ìŠ¤
                doc = self.documents[idx]
                results.append({
                    "content": doc["content"],  # ì‹¤ì œ í…ìŠ¤íŠ¸ ë‚´ìš©
                    "source": {  # ì¶œì²˜ ì •ë³´ êµ¬ì¡°í™”
                        "source": doc["source"],
                        "drug_name": doc["drug_name"],
                        "category": doc["category"],
                        "url": doc.get("url", ""),
                        "company_name": doc.get("company_name", "")
                    },
                    "similarity_score": float(score),
                    "rank": i + 1
                })
        
        return results

    def search_with_api(self, query: str) -> List[Dict]:
        """ì‹¤ì‹œê°„ ì‹ì•½ì²˜ API ê²€ìƒ‰ (ìƒˆë¡œìš´ ì•½ë¬¼ ì§ˆë¬¸ ì‹œ)"""
        try:
            print(f"ì‹¤ì‹œê°„ ì‹ì•½ì²˜ API ê²€ìƒ‰: '{query}'")
            api_results = search_medical_data(query)
            
            if not api_results:
                return []
            
            # API ê²°ê³¼ë¥¼ RAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            formatted_results = []
            for doc in api_results:
                formatted_results.append({
                    "content": doc["content"],
                    "source": {
                        "source": doc["source"],
                        "drug_name": doc["drug_name"],
                        "category": doc["category"],
                        "url": doc.get("url", ""),
                        "company_name": doc.get("company_name", "")
                    },
                    "similarity_score": 0.9,  # API ê²€ìƒ‰ì€ ë†’ì€ ì ìˆ˜
                    "rank": len(formatted_results) + 1
                })
            
            return formatted_results
            
        except Exception as e:
            print(f"ì‹¤ì‹œê°„ API ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
        
    def calculate_similarity(self, query:str, document: str) -> float:
        """ì¿¼ë¦¬ê³¼ ë¬¸ì„œ ê°„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # ì¿¼ë¦¬ ë¬¸ì„œë¥¼ ê°ê° ì„ë² ë”©
            query_embedding = self.embedder.encode([query])
            doc_embedding = self.embedder.encode([document])

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            query_norm = query_embedding / np.linalg.norm(query_embedding)
            doc_norm = doc_embedding / np.linalg.norm(doc_embedding)
            similarity = np.dot(query_norm, doc_norm.T)[0][0]

            return float(similarity)
        
        except Exception as e:
            print(f"ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
            return 0.0
        
    def rank_by_similarity(self, query:str, documents: List[Dict]) -> List[Dict]:
        """ì‹¤ì‹œê°„ ë²¡í„° ìœ ì‚¬ë„ë¡œ ë¬¸ì„œ ì¬ìˆœìœ„í™”"""
        if not documents:
            return []
        
        print("ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚° ì¤‘...")
        scored_documents = []

        for doc in documents:
            similarity_score = self.calculate_similarity(query, doc["content"])
            doc_copy = doc.copy()
            doc_copy["similarity_score"] = similarity_score
            scored_documents.append(doc_copy)
        
        # ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        scored_documents.sort(key=lambda x:x["similarity_score"], reverse=True)

        # ìˆœìœ„ ì¶”ê°€
        for i, doc in enumerate(scored_documents):
            doc["rank"] = i + 1

        return scored_documents
            
    def generate_response_with_sources(self, query: str, search_results: List[Dict]) -> Dict:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ OpenAIë¡œ ì‘ë‹µ ìƒì„±"""

        if not search_results:
            return {
                "response": "ê´€ë ¨ëœ ì˜ë£Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "search_results": [],
                "model_used": "no_results"
            }
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±
        context = ""
        sources_info = []
        
        for i, result in enumerate(search_results, 1):
            source_info = result['source']
            context += f"\n[ë¬¸ì„œ {i}] {result['content']}\n"
            context += f"ì¶œì²˜: {source_info['source']}\n"
            context += f"ì•½ë¬¼: {source_info['drug_name']}\n"
            context += f"ì¹´í…Œê³ ë¦¬: {source_info['category']}\n"
           
            sources_info.append({
                "rank": i,
                "source": source_info['source'],
                "drug_name": source_info['drug_name'],
                "category": source_info['category'],
                "company_name": source_info.get('company_name', ''),
                "similarity": result['similarity_score'],
                "url": f"https://nedrug.mfds.go.kr/search?keyword={urllib.parse.quote(source_info['drug_name'])}"
            })

            # ğŸ” ë””ë²„ê¹…ìš© ì¶œë ¥
            print(f"ğŸ”— ìƒì„±ëœ URL: https://nedrug.mfds.go.kr/search?keyword={urllib.parse.quote(source_info['drug_name'])}")
        
        # OpenAI í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = """ë‹¹ì‹ ì€ ì•½í•™ ì •ë³´ ì œê³µ ì „ë¬¸ AIì•¼. ë‹¤ìŒ ê·œì¹™ì„ ì—„ê²©íˆ ì¤€ìˆ˜í•´ì„œ ë‹µë³€í•´ì¤˜.:

ğŸ” **ê²€ìƒ‰ ê¸°ë°˜ ì‘ë‹µ ì›ì¹™**:
1. ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ì—ë§Œ ê¸°ë°˜í•˜ì—¬ ë‹µë³€í•´ì¤˜.
2. ì¶”ì²œ ì•½ë¬¼ì€ ê°€ì¥ í”í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì•½ë¬¼(ì•½êµ­ì—ì„œ êµ¬í•  ìˆ˜ ìˆëŠ” ì•½ë¬¼)ë¡œ ì¶”ì²œí•´ì¤˜.
3. ê²€ìƒ‰ì´ ë‚˜ì™”ìœ¼ë©´ ì¶”ì²œ ì•½ë¬¼ëª…ì´ ë¬´ì¡°ê±´ ìˆì–´ì•¼í•´. ì¶”ì²œ ì•½ë¬¼ì´ ì—†ë‹¤ë©´ ëª¨ë“  ë‹µë³€ì„ í•  ìˆ˜ê°€ ì—†ì–´.
4. ì‚¬ìš©ìê°€ ì•½ë¬¼ì„ ê²€ìƒ‰í–ˆë‹¤ë©´ 1ë²ˆ ì‘ë‹µ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ê³ , ì‚¬ìš©ìê°€ ì•½ë¬¼ëª…ì„ ì œì™¸í•˜ê³  ê²€ìƒ‰í–ˆë‹¤ë©´ 2ë²ˆ ì‘ë‹µìœ¼ë¡œ ë‹µë³€í•´ì¤˜.
5. ëª¨ë“  ì˜í•™ì  ì •ë³´ëŠ” ì¤‘ë³µì„ ì œê±°í•´ì„œ ë°˜ë“œì‹œ ì¶œì²˜ë¥¼ ëª…í•´ì¤˜.
6. ì‘ê¸‰ìƒí™©(ë‡Œì¡¸ì¤‘, ì‹¬ê·¼ê²½ìƒ‰ ë“± ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ì¦ìƒ) ì‹œ ì¦‰ì‹œ 119 ì—°ë½ ë° ë³‘ì› ë°©ë¬¸ì„ ê¶Œí•´ì¤˜.
7. ì‚¬ëŒë“¤ì´ í•œ ëˆˆì— ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì„¤ëª…í•´ì¤˜. 
8. ë§Œì•½ ì¶”ì²œ ì•½ë¬¼ì„ ì ì„ ìˆ˜ ì—†ë‹¤ë©´, ê·¸ëƒ¥ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë§í•´ì¤˜.
9. ëª¨ë“  ë‹µë³€ ëì— "âš ï¸ ì´ ì •ë³´ëŠ” ì˜ë£Œì§„ ìƒë‹´ì„ ëŒ€ì²´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤." í¬í•¨í•´ì¤˜.
10. ì§„ë‹¨ì´ë‚˜ ì²˜ë°©ì€ ì ˆëŒ€ í•˜ì§€ë§ˆ.

# 1ë²ˆ ì‘ë‹µ êµ¬ì¡°:
ğŸ’Š ê²€ìƒ‰ ì•½ë¬¼ : (ê²€ìƒ‰ ì•½ë¬¼ëª…)\n
1. ì•½ì˜ íš¨ëŠ¥ (ëª…í™•í•˜ê²Œ ë‹¨ì–´ë¡œ ë‚˜ì—´í•´ì¤˜. ì˜ˆ: ì½§ë¬¼, ì¬ì±„ê¸°, ë°œì—´)\n
2. ìƒì„¸ ì •ë³´
    - ìš©ë²•: (ì•½ë¬¼ ì‚¬ìš©ë²•ì„ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì ì–´ì¤˜.)
    - ìµœëŒ€ ìš©ëŸ‰
    - ë³µìš© ì‹œ ì£¼ì˜ì‚¬í•­
    - ë¶€ì‘ìš©
    - ë³´ê´€ë²•

# 2ë²ˆ ì‘ë‹µ êµ¬ì¡°:
ğŸ’Š ì¦ìƒ : (ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì¦ìƒì„ ê°€ëŠ¥í•˜ë©´ ì „ë¬¸ìš©ì–´ë¡œ ìš”ì•½í•´ì„œ ì ì–´ì¤˜. ì˜ˆ: ë¨¸ë¦¬ê°€ ì•„íŒŒ -> ë‘í†µ)\n
1. ì¶”ì²œ ì•½ë¬¼: (ì¶”ì²œ ì•½ë¬¼ëª…: ë¬´ì¡°ê±´ ìˆì–´ì•¼í•´. ë§Œì•½ ì—†ë‹¤ë©´ ë‚˜ë¨¸ì§€ë„ ë‹µë³€í•  ìˆ˜ê°€ ì—†ëŠ”ê±°ì•¼.)\n
2. ì•½ì˜ íš¨ëŠ¥: (ëª…í™•í•˜ê²Œ ë‹¨ì–´ë¡œ ë‚˜ì—´í•´ì¤˜. ì˜ˆ: ì½§ë¬¼, ì¬ì±„ê¸°, ë°œì—´)\n
3. ìƒì„¸ ì •ë³´
    - ìš©ë²•: (ì•½ë¬¼ ì‚¬ìš©ë²•ì„ ì•Œì•„ë³´ê¸° ì‰½ê²Œ ì ì–´ì¤˜.)
    - ìµœëŒ€ ìš©ëŸ‰
    - ë³µìš© ì‹œ ì£¼ì˜ì‚¬í•­
    - ë¶€ì‘ìš©
    - ë³´ê´€ë²•
"""

        user_prompt = f"""ì§ˆë¬¸: {query}

ì°¸ê³  ë¬¸ì„œ:
{context}

ìœ„ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ì•ˆì „í•œ ë‹µë³€ì„ ì œê³µí•´ì¤˜."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=800,
                temperature=0.1  # ì¼ê´€ëœ ì‘ë‹µì„ ìœ„í•´ ë‚®ì€ temperature
            )
            
            ai_response = response.choices[0].message.content
            
            return {
                "response": ai_response,
                "sources": sources_info,
                "search_results": search_results,
                "model_used": "gpt-4o-mini",
                "usage": {
                    "prompt_tokens": response.usage.prompt_tokens,
                    "completion_tokens": response.usage.completion_tokens,
                    "total_tokens": response.usage.total_tokens
                }
            }
            
        except Exception as e:
            print(f"OpenAI API ì˜¤ë¥˜: {e}")
            return {
                "response": "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ëœ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "sources": sources_info,
                "search_results": search_results,
                "error": str(e)
            }
    
    def process_query(self, query: str) -> Dict:
        """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        
        # 1. ë²¡í„° ì¸ë±ìŠ¤ì—ì„œ ë¬¸ì„œ ê²€ìƒ‰
        vector_results = self.search_documents(query, top_k=3)

        # 2. # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì‹¤ì‹œê°„ api ê²€ìƒ‰ 
        api_results = []
        # if len(vector_results) < 2:   
        #     api_results = self.search_with_api(query)
        #     # ì‹¤ì‹œê°„ ê²€ìƒ‰ ê²°ê³¼ë„ ìœ ì‚¬ë„ë¡œ ì¬ìˆœìœ„í™”
        #     api_results = self.rank_by_similarity(query, api_results) 
        # ìœ ì‚¬ë„ 0.7 ì´í•˜ì´ê±°ë‚˜ ê²°ê³¼ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ API ê²€ìƒ‰
        
        low_similarity = any(result['similarity_score'] < 0.7 for result in vector_results)
        if len(vector_results) < 2 or low_similarity:
            print(f"ğŸŒ API ê²€ìƒ‰ íŠ¸ë¦¬ê±°: ë²¡í„°ê²°ê³¼={len(vector_results)}, ë‚®ì€ìœ ì‚¬ë„={low_similarity}")
            api_results = self.search_with_api(query)
            api_results = self.rank_by_similarity(query, api_results) 

        # 3. ê²°ê³¼ ì¡°í•© (ë²¡í„° ê²€ìƒ‰ ìš°ì„ , api ê²€ìƒ‰ ë³´ì™„)
        all_results = vector_results +  api_results

        # 4. ìƒìœ„ 3ê°œë§Œ ì„ íƒ
        if len(all_results) > 3:
            all_results = all_results[:3]
            
        # 5. ìˆœìœ„ ì¬ì¡°ì •
        for i, result in enumerate(all_results):
            result['rank'] = i + 1
        
        if not all_results:
            return {
                "response": "ê´€ë ¨ëœ ì˜ë£Œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì•½ë¬¼ëª…ì´ë‚˜ ì§ˆë¬¸ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”.",
                "sources": [],
                "search_results": [],
                "model_used": "no_results"
            }
        
        # 6. OpenAIë¡œ ì‘ë‹µ ìƒì„±
        response_data = self.generate_response_with_sources(query, all_results)
        
        print(f"ê²€ìƒ‰ ì™„ë£Œ: ë²¡í„° {len(vector_results)}ê°œ + API {len(api_results)}ê°œ")

        return response_data
    
# ì „ì—­ RAG ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
rag_system = None

def get_rag_system():
    """RAG ì‹œìŠ¤í…œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global rag_system
    if rag_system is None:
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        rag_system = MedicalRAGSystem()
        print("RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    return rag_system